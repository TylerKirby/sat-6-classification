{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Classifier for SAT-6\n",
    "===\n",
    "Todo\n",
    "---\n",
    "- [ ] Log learning rate, loss, and accuracy to `TensorBoardX` along with standard classification stats (recall, precision, f1, confusion matrix etc.)\n",
    "- [ ] Implement stochastic gradient descent with warm restarts\n",
    "- [ ] Implement test time augmentation\n",
    "- [ ] Implement early stopping ans save best weights\n",
    "- [ ] Achieve overall accuracy of 98.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision as tv\n",
    "import scipy.io\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/tyler/Datasets/deepsat-sat6/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_test_sat6.csv',\n",
       " 'sat-6-full.mat',\n",
       " 'X_train_sat6.csv',\n",
       " 'sat6annotations.csv',\n",
       " 'y_train_sat6.csv',\n",
       " 'X_test_sat6.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, X_csv, y_csv, sample_size=1000):\n",
    "        if sample_size:\n",
    "            self.instances = pd.read_csv(X_csv, nrows=sample_size)\n",
    "            self.labels = pd.read_csv(y_csv, nrows=sample_size)\n",
    "        else:\n",
    "            self.instances = pd.read_csv(X_csv)\n",
    "            self.labels = pd.read_csv(y_csv)\n",
    "\n",
    "    def __len__(self): return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.instances.iloc[idx].values.reshape(-1, 4, 28, 28).clip(0, 255).astype(np.uint8).squeeze(axis=0)\n",
    "        label = self._get_labels(self.labels.iloc[idx].values)\n",
    "        return {'chip': img, 'label': label}\n",
    "\n",
    "    def _get_labels(self, row_values):\n",
    "        annotations = {'building': 0, 'barren_land': 1,'trees': 2, 'grassland': 3, 'road': 4, 'water': 5}\n",
    "        labels = [list(annotations.values())[i] for i, x in enumerate(row_values) if x == 1]\n",
    "        return labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = SatelliteDataset(X_csv=data_dir+'X_train_sat6.csv', y_csv=data_dir+'y_train_sat6.csv')\n",
    "validation_set = SatelliteDataset(X_csv=data_dir+'X_test_sat6.csv', y_csv=data_dir+'y_test_sat6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tv.transforms.Compose([\n",
    "    tv.transforms.RandomHorizontalFlip(),\n",
    "    tv.transforms.RandomVerticalFlip(),\n",
    "    tv.transforms.RandomRotation(90),\n",
    "    tv.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    training_set,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    validation_set,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(t.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallCNN, self).__init__()\n",
    "        self.conv1 = t.nn.Sequential(\n",
    "            t.nn.Conv2d(\n",
    "                in_channels=4,\n",
    "                out_channels=32,\n",
    "                kernel_size=3\n",
    "            ),\n",
    "            t.nn.BatchNorm2d(32),\n",
    "            t.nn.ReLU(),\n",
    "            t.nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=32,\n",
    "                kernel_size=3\n",
    "            ),\n",
    "            t.nn.BatchNorm2d(32),\n",
    "            t.nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = t.nn.Sequential(\n",
    "            t.nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3\n",
    "            ),\n",
    "            t.nn.BatchNorm2d(64),\n",
    "            t.nn.ReLU(),\n",
    "            t.nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size=3\n",
    "            ),\n",
    "            t.nn.BatchNorm2d(64),\n",
    "            t.nn.ReLU(),\n",
    "        )\n",
    "        self.pool = t.nn.MaxPool2d(2)\n",
    "        self.fc1 = t.nn.Linear(1024, 512)\n",
    "        self.bn = t.nn.BatchNorm1d(512)\n",
    "        self.drop_out = t.nn.Dropout2d(0.2)\n",
    "        self.fc2 = t.nn.Linear(512, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn(x)\n",
    "        x = t.nn.functional.relu(x)\n",
    "        x = self.drop_out(x)\n",
    "        output = self.fc2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SmallCNN()\n",
    "optim = t.optim.Adam(model.parameters())\n",
    "crit = t.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model_name):\n",
    "    for e in tqdm(range(epochs)):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_loss = 0\n",
    "        for s, inst in enumerate(train_loader):\n",
    "            chip = inst['chip'].float()\n",
    "            output = model(chip)\n",
    "            loss = crit(output, inst['label'])\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            train_loss += loss\n",
    "            _, predicted = t.max(output.data, 1)\n",
    "            train_correct += (predicted == inst['label']).sum().item()\n",
    "    \n",
    "        writer.add_scalars('loss', {'training_loss': loss / len(training_set)}, e)\n",
    "        writer.add_scalars('acc', {'train_acc': train_correct / len(training_set)}, e)\n",
    "            \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_loss = 0\n",
    "        for s, inst in enumerate(val_loader):\n",
    "            chip = inst['chip'].float()\n",
    "            output = model(chip)\n",
    "            loss = crit(output, inst['label'])\n",
    "            \n",
    "            val_loss += loss\n",
    "            _, predicted = t.max(output.data, 1)\n",
    "            val_correct = (predicted == inst['label']).sum().item()\n",
    "            \n",
    "        writer.add_scalars('loss', {'validation_loss': val_loss / len(validation_set)}, e)\n",
    "        writer.add_scalars('acc', {'val_acc': val_correct / len(validation_set)})\n",
    "        \n",
    "    t.save(model.state_dict(), '{}.pt'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:13<00:00,  4.44s/it]\n"
     ]
    }
   ],
   "source": [
    "train(3, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
